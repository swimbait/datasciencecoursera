---
title: "coursera_ML"
author: "swimbait"
date: "November 4, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xgboost)
setwd("C:/Users/Emmert/Documents/R")
```

## Read in and format the data

The training and test data are read in, and missing values are set to -99999, which is acceptable since xgboost is being used. Then the response is converted to a numeric from zero to four for the multiclass machine learning. Last, the columns are matched between train and test.

```{r}
train<-read.csv("pml-training.csv")
test<-read.csv("pml-testing.csv")

train<-data.frame(train)
test<-data.frame(test)

train[is.na(train)]<-(-99999)
test[is.na(test)]<-(-99999)

trainResponse<-train$classe
trainResponseF<-as.numeric(as.factor(trainResponse))-1

train<-train[,which(colnames(train) %in% colnames(test))]
test<-test[,which(colnames(test) %in% colnames(train))]

```

## Cross validation using xgboost

First, initialize the xgboost settings. Then, an index of 5 values is assigned to the training data to facilitate k = 5 fold cross validation. A function is created to conduct the training and evaluation on the validation set for each fold, the output is a list containing the confusion matrix of the predicted response compared to the actual response along with the misclassification error rate (mer). This is processed on each fold and the resulting error rate is very low, with only several mistakes made across the entire training set of almost 20,000 records. 

```{r}
param = list(objective = 'multi:softprob',
             num_class = 5,
             eta = 0.01,
             "subsample" = 0.7,
             "colsample_bytree" = 0.5,
             "min_child_weight" = 6,
             max_depth = 4,
             eval_metric = 'mlogloss')

set.seed(999)
kInd<-as.integer(runif(nrow(train),1,6))

crossVal<-function(trainInd,valInd){
  dtrain<-xgb.DMatrix(data.matrix(train[kInd %in% trainInd,]), label=trainResponseF[kInd %in% trainInd])
  dval<-xgb.DMatrix(data.matrix(train[kInd==valInd,]), label=trainResponseF[kInd==valInd])
  gc()
  watchlist<-list(train = dtrain, eval = dval)
  clf<-xgb.train(   params = param,
                    data= dtrain,
                    nrounds = 600,
                    verbose = 0,
                    early_stopping_rounds = 5,
                    watchlist = watchlist,
                    print_every_n = 5,
                    maximize = FALSE)
  pred.val<-predict(clf,data.matrix(train[kInd==valInd,]),reshape=T)
  finalPred<-apply(pred.val,1,FUN=which.max)
  confusion<-table(chartr("12345","ABCDE",trainResponseF[kInd==valInd]+1),chartr("12345","ABCDE",finalPred))
  list(confusion=confusion,mer=(sum(confusion)-sum(diag(confusion)))/sum(confusion))
}

k1<-crossVal(c(1,2,3,4),5)
k2<-crossVal(c(2,3,4,5),1)
k3<-crossVal(c(1,3,4,5),2)
k4<-crossVal(c(1,2,4,5),3)
k5<-crossVal(c(1,2,3,5),4)

k1
k2
k3
k4
k5
```

## Prediction on test data set

The entire training set is used here to train an xgboost with the same settings as the cross validation. Then it is applied to the test set. 

```{r}
dtrain<-xgb.DMatrix(data.matrix(train), label=trainResponseF)
watchlist<-list(train = dtrain, eval = dtrain)
clf<-xgb.train(   params = param,
                  data= dtrain,
                  nrounds = 600,
                  verbose = 0,
                  early_stopping_rounds = 5,
                  watchlist = watchlist,
                  print_every_n = 5,
                  maximize = FALSE)
pred.val<-predict(clf,data.matrix(test),reshape=T)
finalPred<-apply(pred.val,1,FUN=which.max)
finalPred<-chartr("12345","ABCDE",finalPred)
cbind(label=test$X,predictedClass=finalPred)
```

